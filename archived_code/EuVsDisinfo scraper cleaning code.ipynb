{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning code for EuVsDisinfo scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "df = pd.read_csv('../datasets/euvsdisinfoheadings.csv')\n",
    "dfpages = pd.read_csv('dfpages.csv')\n",
    "dfpages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pages that didn't pull data\n",
    "\n",
    "Fixed - running the scraper code above shouldn't create any missed pages any more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the pages that failed to scrape\n",
    "noscrapes = df[df.index.isin(dfpages[dfpages['Id'].isnull()].index.to_list())]\n",
    "noscrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newpages = []\n",
    "for i, url in enumerate(noscrapes['NATO_url'].to_list()):\n",
    "    if int(i/10) == i/10:\n",
    "        print('{} {}'.format(i, url))\n",
    "    try:\n",
    "        html = requests.get(url).text\n",
    "        soup = bs(html, 'lxml')\n",
    "        bodyclasses = soup.find('body').get('class')\n",
    "        pdata = [url,\n",
    "                 bodyclasses[[i for i, s in enumerate(bodyclasses) if s.startswith('postid-')][0]],\n",
    "                 soup.find('h1', 'b-catalog__report-title').text,\n",
    "                 soup.find('div', 'b-report__summary-text').text.strip(),\n",
    "                 soup.find('div', 'b-report__disproof-text').text.strip()]\n",
    "        if soup.find('div', 'b-catalog__link') != None:\n",
    "            pdata += [[{x.text: x.get('href')} for x in soup.find('div', 'b-catalog__link').find_all('a')],\n",
    "                     [bb.text.strip() for bb in soup.find('ul', 'b-catalog__repwidget-list').find_all('li')]]\n",
    "        else:\n",
    "            if soup.find('div', 'b-catalog__repwidget-source').find('a') != None:\n",
    "                pdata += [[x.get('href') for x in soup.find('div', 'b-catalog__repwidget-source').find_all('a')]]\n",
    "            else:\n",
    "                pdata += [[]]\n",
    "            pdata += [[bb.text.strip() for bb in soup.find('ul', 'b-catalog__repwidget-list').find_all('li')]]\n",
    "    except:\n",
    "        print('Scrape failed for {}'.format(url))\n",
    "        pdata = [url]\n",
    "    newpages += [pdata]\n",
    "\n",
    "dfnewpages = pd.DataFrame(newpages, columns=['NATO_url', 'Id', 'PageTitle', 'Summary', 'Disproof', 'Media', 'Details'])\n",
    "dfnewpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('euvsdisinfoheadings.csv')\n",
    "df = pd.concat([df, dfpages], axis=1, sort=False)\n",
    "df = pd.merge(df, dfnewpages, how='left', on=['NATO_url'], suffixes=['','_new'])\n",
    "for col in ['PageTitle', 'Summary', 'Disproof', 'Media', 'Details', 'Id']:\n",
    "    df.loc[df['Id'].isnull(), col] = df.loc[df['Id'].isnull(), col+'_new']\n",
    "df.drop(['Id_new', 'PageTitle_new', 'Summary_new', 'Disproof_new', 'Media_new', 'Details_new'], axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying details entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deetlist = df['Details'].to_list()\n",
    "deets = {i:{dd.split(':')[0]:dd.split(':')[1].strip() for dd in deet} for i, deet in enumerate(deetlist)}\n",
    "dfdeets = pd.DataFrame.from_dict(deets).transpose()\n",
    "dfdeets.rename(columns={x:\"detail_\"+x for x in dfdeets.columns}, inplace=True)\n",
    "df = pd.concat([df, dfdeets], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check data columns - remove anything redundant\n",
    "\n",
    "No idea where column 'Index' came from\n",
    "\n",
    "These are the same: \n",
    "* df[df['detail_DATE OF PUBLICATION'] != df['Date'].str.replace('.', '/')]\n",
    "* df[(df['detail_Country'] != df['Country']) & (df['Country'].notnull())]\n",
    "* Title and the second half (after the :) of PageTitle\n",
    "* Outlets is better-formed than detail_Outlets (which has text truncated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Index', 'Details', 'detail_DATE OF PUBLICATION', 'detail_Country', 'Title', \n",
    "         'detail_Outlet'], axis=1, inplace=True)\n",
    "df.rename(columns={'Date':'Date of publication', 'detail_Keywords':'Keywords', \n",
    "                   'detail_Language/target audience':'Language/target audience', \n",
    "                   'detail_Reported in':'Reported in'}, inplace=True)\n",
    "df[['Type', 'Title']] = pd.DataFrame(df['PageTitle'].str.split(':', n=1).to_list(), columns=['Type', 'Title'])\n",
    "df['Title'] = df['Title'].str.strip()\n",
    "df.drop(['PageTitle'], axis=1, inplace=True)\n",
    "df.to_csv('euvsdisinfodata.csv', index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URLs that didn't get pulled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Media'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsplit = pd.DataFrame(df['Media'].to_list(), columns=['Url1', 'Url2'])\n",
    "df = pd.concat([df, urlsplit], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only 52 of these - none of these have an archive link on the page\n",
    "#Leave as-is\n",
    "df[(df['Url2'].isnull()) & (df['Url1'].apply(type) == dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most of these have 1 or more original links, but no archive link\n",
    "df[(df['Url2'].isnull()) & (df['Url1'].apply(type) != dict)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['Url2'].isnull()) & (df['Url1'].apply(type) != dict)]['Media'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Url1'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['NATO_url']=='https://euvsdisinfo.eu/report/strange-coincidence-that-40-areas-are-mostly-affected-by-coronavirus/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latest version of page pull code\n",
    "html = requests.get(url).text\n",
    "soup = bs(html, 'lxml')\n",
    "bodyclasses = soup.find('body').get('class')\n",
    "pdata = [url,\n",
    "         bodyclasses[[i for i, s in enumerate(bodyclasses) if s.startswith('postid-')][0]],\n",
    "         soup.find('h1', 'b-catalog__report-title').text,\n",
    "         soup.find('div', 'b-report__summary-text').text.strip(),\n",
    "         soup.find('div', 'b-report__disproof-text').text.strip()]\n",
    "if soup.find('div', 'b-catalog__link') != None:\n",
    "    pdata += [[{x.text: x.get('href')} for x in soup.find('div', 'b-catalog__link').find_all('a')],\n",
    "             [bb.text.strip() for bb in soup.find('ul', 'b-catalog__repwidget-list').find_all('li')]]\n",
    "else:\n",
    "    if soup.find('div', 'b-catalog__repwidget-source').find('a') != None:\n",
    "        pdata += [[x.get('href') for x in soup.find('div', 'b-catalog__repwidget-source').find_all('a')]]\n",
    "    else:\n",
    "        pdata += [[]]\n",
    "    pdata += [[bb.text.strip() for bb in soup.find('ul', 'b-catalog__repwidget-list').find_all('li')]]\n",
    "pdata"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
